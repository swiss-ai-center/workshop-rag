{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome\n",
    "\n",
    "Authors:\n",
    "- Célien Donzé, research assistant at Haute Ecole Arc Ingénierie, Switzerland\n",
    "- Jonathan Guerne, research assistant at Haute Ecole Arc Ingénierie, Switzerland\n",
    "- Henrique Marques Reis, research assistant at Haute Ecole Arc Ingénierie, Switzerland\n",
    "- Pedro Costa, CO-Founder and CTO at Lumind, Switzerland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install langchain langchain-community faiss-cpu pymupdf pypdf sentence_transformers rich wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.chains import RetrievalQA, LLMChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms.ollama import Ollama\n",
    "import os\n",
    "import json\n",
    "from rich.console import Console\n",
    "import zipfile\n",
    "import os\n",
    "import wget\n",
    "\n",
    "\n",
    "console=Console()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the \"data/PDFs\" folder if it doesn't exist\n",
    "os.makedirs(\"data/PDFs\", exist_ok=True)\n",
    "\n",
    "# Download the PDFs\n",
    "url = \"https://www.dropbox.com/scl/fo/xhqjzofiqnbmraxksgvlh/AAoL_WMBFOYDuipk5T_tTus?rlkey=qbbcvw4gbw6bpxkeijt6m94kt&st=yhap82wh&dl=1\"\n",
    "filename=wget.download(url, \".\")\n",
    "\n",
    "zip_file_path = f\"./{filename}\"\n",
    "extract_folder = \"data/PDFs\"\n",
    "\n",
    "# Create the extract folder if it doesn't exist\n",
    "os.makedirs(extract_folder, exist_ok=True)\n",
    "\n",
    "# Extract the zip file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_folder)\n",
    "\n",
    "print(\"Pdf file downloaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "- [langchain](https://python.langchain.com/v0.1/docs/get_started/introduction/)\n",
    "- [Ollama website](https://ollama.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_ADDRESS = \"http://XXX.XX.XX.XX:11434\"\n",
    "LLM_NAME=\"gemma\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm= Ollama(\n",
    "        model=LLM_NAME,\n",
    "        base_url=OLLAMA_ADDRESS,\n",
    "        temperature=0.1, # Will be explained later\n",
    "        stop=[\"<end_of_turn>\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a prompt\n",
    "\n",
    "A prompt is generally divided into two parts: the context and the question.\n",
    "\n",
    "The context provides the information that the model will use to generate its answer, while the question specifies what the model is expected to respond to.\n",
    "\n",
    "In a prompt, special characters are used to delineate different sections. For instance, in Gemma, these are `<start_of_turn>` and `<end_of_turn>`.\n",
    "\n",
    "Additionally, LangChain requires markers indicating where to insert the user's question and the context retrieved from documents. For the question.\n",
    "\n",
    "Gemma prompt template :\n",
    "\n",
    "```html\n",
    "<start_of_turn>user\n",
    "{{ if .System }}{{ .System }} {{ end }}{{ .Prompt }}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "{{ .Response }}<end_of_turn>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"<start_of_turn>\n",
    "You are an helpful assistant that answer the question in detail.\n",
    "\n",
    "Human input: {question}<end_of_turn>\n",
    "<start_of_turn>Assistant:<end_of_turn>\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"question\"], template=template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the chain and start a conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    # verbose=True, # uncomment if you want to see more information about the chain\n",
    "    prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = conversation.invoke(input=\"What is the capital of Switzerland?\")\n",
    "console.print(result.get(\"text\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(\"./\", \"data\")\n",
    "PDF_DIR = os.path.join(DATA_DIR, \"PDFs\")\n",
    "VECTORSTORES_DIR = os.path.join(DATA_DIR, \"vectorstores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFDirectoryLoader(PDF_DIR)\n",
    "doc = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding a PDF in a vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 100\n",
    "EMBEDDING_MODEL_NAME = \"BAAI/bge-large-en-v1.5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"chunk_overlap_size_scheme.png\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=..., chunk_overlap=...\n",
    ")\n",
    "\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "embedding_model = HuggingFaceBgeEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits = text_splitter.split_documents(...)\n",
    "vectorstore = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.save_local(VECTORSTORES_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is temperature?\n",
    "\n",
    "The temperature parameter in a language model (LLM) controls the randomness of the model's output.\n",
    "\n",
    "A lower temperature value (closer to 0) makes the model more deterministic, favoring higher probability words and resulting in more predictable and repetitive text.\n",
    "\n",
    "A higher temperature value (closer to 1) increases randomness, allowing for more creative and diverse responses by giving less probable words a better chance of being chosen.\n",
    "\n",
    "Adjusting the temperature helps balance between coherence and creativity in the generated text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New prompt\n",
    "\n",
    "In RAG we need to add another marker to indicate where the new information (or context) should be inserted for this we use the variable named `{context}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"...\"\"\"\n",
    "\n",
    "\n",
    "prompt_template = PromptTemplate(input_variables=[...],template=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top k of chunks to retrieve from the vectorstore\n",
    "NB_RETRIVED_CHUNKS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rqa_chain = RetrievalQA.from_chain_type(\n",
    "        llm,\n",
    "        retriever=vectorstore.as_retriever(search_kwargs={\"k\": NB_RETRIVED_CHUNKS}),\n",
    "        chain_type_kwargs={\"prompt\": ...},\n",
    "        input_key=..., # same as the variable in the prompt\n",
    "        output_key=\"answer\",\n",
    "        return_source_documents=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatting with a pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ...\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embellishing the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_document(x):\n",
    "        return x if x is None else os.path.basename(x)\n",
    "\n",
    "def prepare_page(x):\n",
    "        return x if x is None else int(x) + 1\n",
    "\n",
    "def prepare_source(x):\n",
    "        return {\n",
    "            \"document\": prepare_document(x.metadata.get(\"source\", None)),\n",
    "            \"page\": prepare_page(x.metadata.get(\"page\", None)),\n",
    "            \"chunk\": x.page_content,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = [prepare_source(x) for x in result[\"source_documents\"]]\n",
    "console.print(json.dumps(sources, indent=1),highlight=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
